{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import text_processing\n",
    "import sys\n",
    "import unicodedata\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import topic_weights\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickleizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subdir = 'final_csvs2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataall = pd.DataFrame.from_csv(os.path.join(subdir,'dataall.csv'),encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordoption = 'lemma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n"
     ]
    }
   ],
   "source": [
    "text = dataall['full_text']\n",
    "tidiedtext = []\n",
    "for i, item in enumerate(text):\n",
    "    newtext = text_processing.tidy_text_noun_adj(item,wordoption=wordoption)\n",
    "    tidiedtext.append(newtext)\n",
    "    if i%100 == 0:\n",
    "        print(i)\n",
    "tidiedtextjoined = [' '.join(text) for text in tidiedtext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tidiedtextjoined2 = [text_processing.remove_punctuation(text) for text in tidiedtextjoined]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatidy = pd.DataFrame(index=dataall.index,columns=[])\n",
    "datatidy['TidiedText'] = tidiedtextjoined2\n",
    "datatidy.to_csv(os.path.join(subdir,'datatidynounadj.csv'),encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datatidy = pd.DataFrame.from_csv(os.path.join(subdir,'datatidy.csv'),encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK/Sk-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 1000\n",
    "n_topics = 50\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "tidiedtextjoined = datatidy['TidiedText']\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95,min_df=2,\n",
    "                                max_features=n_features)\n",
    "tf = tf_vectorizer.fit_transform(tidiedtextjoined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_jobs=1, n_topics=50, perp_tol=0.1, random_state=0,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=10,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "sexual sex men assault abuse woman discrimination behavior identity social relationship man young facility treatment partner adult right body report\n",
      "Topic #1:\n",
      "health care patient doctor medical hospital cancer disease ebola case insurance treatment people cost center service many month new death\n",
      "Topic #2:\n",
      "school student college education university teacher high percent class year program many public state job kid rate degree child parent\n",
      "Topic #3:\n",
      "drug treatment use mental people health disease year pain medical high state new risk trial many rate death american study\n",
      "Topic #4:\n",
      "european europe union french france germany german country paris britain minister member crisis le national leader year state new front\n",
      "Topic #5:\n",
      "water climate energy gas oil environmental change emission year state global carbon plant power new fuel air natural river industry\n",
      "Topic #6:\n",
      "food animal park land human year scientist area mile site space village many science place new nature field natural california\n",
      "Topic #7:\n",
      "time day year life people first white friend way man thing new its black world old something night home house\n",
      "Topic #8:\n",
      "state military war iraq syria islamic force isi united group syrian army turkey attack troop afghanistan terrorist american security civilian\n",
      "Topic #9:\n",
      "people life year risk many age study time likely percent brain change level young problem effect number way group early\n",
      "Topic #10:\n",
      "mr sunday president tuesday office wednesday political charge fact year supporter former speech leader many friday position personal call message\n",
      "Topic #11:\n",
      "clinton hillary sander mrs democratic campaign candidate voter president bill democrat primary presidential state secretary obama speech political wall email\n",
      "Topic #12:\n",
      "marriage church gay religious christian catholic samesex pope religion couple god faith conservative many issue last american love people family\n",
      "Topic #13:\n",
      "government minister political party india prime leader election president indian year last protest opposition medium power pakistan mr parliament new\n",
      "Topic #14:\n",
      "mission domestic serious relationship settlement department paul george conservative city home decision youre parliament difficult figure feeling japan authority health\n",
      "Topic #15:\n",
      "trump republican candidate bush cruz presidential donald rubio debate campaign hes president trumps governor time christie way iowa its senator\n",
      "Topic #16:\n",
      "worker wage job labor minimum work employee hour pay employer union state employment new higher business force increase industry federal\n",
      "Topic #17:\n",
      "campaign money political group election candidate donor ad public organization commission politics finance spending corruption citizen fund interest independent politician\n",
      "Topic #18:\n",
      "united country nation refugee people state african border international immigration immigrant world year last migrant human many right number camp\n",
      "Topic #19:\n",
      "police black officer violence crime death victim force murder community people case racial attack justice video violent arrest criminal city\n",
      "Topic #20:\n",
      "law state right act federal government people coverage protection public exchange affordable plan insurance legal freedom legislature discrimination amendment states\n",
      "Topic #21:\n",
      "its people thing im way thats lot good something medium news theyre many much dont big show kind were ive\n",
      "Topic #22:\n",
      "party voter republican political election conservative democratic white liberal democrat politics vote percent candidate issue right policy poll national support\n",
      "Topic #23:\n",
      "study research game team player university sport test researcher science field professor professional performance national academic association journal skill practice\n",
      "Topic #24:\n",
      "time book story news article editor reader journalist reporter newspaper the writer page ms email author new press magazine medium\n",
      "Topic #25:\n",
      "woman men abortion female ms gender womens male girl birth young year percent right work role life many partner issue\n",
      "Topic #26:\n",
      "mr city state trump budget new york cruz woman year question first wife picture time cut public image need much\n",
      "Topic #27:\n",
      "people world society muslim many culture life way social moral history century idea the power today new cultural political american\n",
      "Topic #28:\n",
      "city housing poor poverty community people home local neighborhood family new building resident town area program public many affordable place\n",
      "Topic #29:\n",
      "china chinese japan chinas north korea beijing south asia united sea state world island official economic claim leader country year\n",
      "Topic #30:\n",
      "child family parent mother father girl kid boy school son young home baby adult daughter age many year birth village\n",
      "Topic #31:\n",
      "israel palestinian israeli jewish jew israels gaza state peace west bank settlement arab minister prime land side right security authority\n",
      "Topic #32:\n",
      "tax percent income program budget year inequality spending american benefit cut government money cost revenue plan federal fund subsidy state\n",
      "Topic #33:\n",
      "south war history texas civil mexico year american southern president county country july king right summer today century april state\n",
      "Topic #34:\n",
      "russia russian ukraine putin ukrainian russias west soviet putins moscow sanction eastern western war state president union world cold united\n",
      "Topic #35:\n",
      "data time technology question work information way people new thing computer many search much idea good different day human important\n",
      "Topic #36:\n",
      "market trade price country global agreement stock world economy deal investor currency capital international dollar united investment exchange product free\n",
      "Topic #37:\n",
      "debt loan greece greek island bank fund crisis financial credit government money new finance country deal economy end term international\n",
      "Topic #38:\n",
      "economic economy policy bank rate growth financial year crisis job interest government good inflation reform economist unemployment problem time market\n",
      "Topic #39:\n",
      "mr school united year state new student percent president security senior next free government administration right important university several human\n",
      "Topic #40:\n",
      "gun weapon christie safety mass violence people law owner ban sale arm control national public death second state year issue\n",
      "Topic #41:\n",
      "company business service industry internet consumer fee rule executive customer corporate inmate private money corporation sale commission firm new product\n",
      "Topic #42:\n",
      "court justice case supreme judge decision ruling appeal state opinion law courts lawyer right legal federal chief constitutional year majority\n",
      "Topic #43:\n",
      "republican bill house congress senate democrat senator president committee obama year representative congressional vote last lawmaker member legislation week new\n",
      "Topic #44:\n",
      "iran nuclear saudi irans iranian arab arabia program middle sunni east world islamic region ally state regional regime power threat\n",
      "Topic #45:\n",
      "government country national system power africa political foreign economic security countrys public official democracy institution citizen leader nation region many\n",
      "Topic #46:\n",
      "new york city state mayor year de blasio cuomo governor public bill plan citys system time gov board authority council\n",
      "Topic #47:\n",
      "law federal department prison state criminal agency report government official year case public enforcement investigation crime legal security information system\n",
      "Topic #48:\n",
      "president obama american united state deal policy america administration foreign washington agreement obamas power americas country sanction leader year issue\n",
      "Topic #49:\n",
      "percent state british american year britain population united survey country number london last poll new national rate people average north\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "topic_weights.print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic_distrib = lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic_distrib /= np.sum(doc_topic_distrib,axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1)\n",
    "ax = fig.gca()\n",
    "ax = ax.matshow(doc_topic_distrib, cmap=plt.cm.gray, aspect = '0.005')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_weights.save_topic_weights(datatidy,doc_topic_distrib,\n",
    "                   stem='lemma_noun_adj',package='sklearn',rows='all',subdir=subdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickleizer.save_topic_analyzer(lda,tf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda, tf_vectorizer = pickleizer.load_topic_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = u'WHEN I moved to Europe 12 years ago, my biggest concern was whether I’d ever speak decent French. Practically every American I knew came to visit, many saying they dreamed of living here, too. I didn’t worry much about far-right political parties, or the European Union. I certainly didn’t fret about terrorism.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHEN I moved to Europe 12 years ago, my biggest concern was whether I’d ever speak decent French. Practically every American I knew came to visit, many saying they dreamed of living here, too. I didn’t worry much about far-right political parties, or the European Union. I certainly didn’t fret about terrorism.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tidiedtext = text_processing.tidy_text_and_join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'move europ year ago biggest concern whether id ever speak decent french practic everi american knew came visit mani say dream live didnt worri much farright polit parti european union certain didnt fret terror'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tidiedtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document_term_matrix = tf_vectorizer.transform([tidiedtext])\n",
    "weights = lda.transform(document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1000 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 28 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsnorm = weights/np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Weight', 0.30488753477496194)\n",
      "get say like one peopl dont go said want thing\n",
      "('Weight', 0.35152753792222802)\n",
      "one live like peopl life world year us day time\n",
      "('Weight', 0.093731672333597207)\n",
      "parti elect polit minist leader nation govern vote presid prime\n",
      "('Weight', 0.13031220950545552)\n",
      "european union europ britain british franc germani minist member nation\n",
      "('Weight', 0.089541045463752925)\n",
      "french attack franc pari terrorist terror secur kill bomb threat\n"
     ]
    }
   ],
   "source": [
    "for topic, weight in zip(lda.components_,weightsnorm[0,:]):\n",
    "    if weight > 0.05:\n",
    "        print('Weight', weight)\n",
    "        print(\" \".join([tf_feature_names[i]\n",
    "            for i in topic.argsort()[:-10 - 1:-1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(13603 unique tokens: [u'yellow', u'narcotic', u'four', u'jihad', u'hanging']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "bad_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq <= 10]\n",
    "dictionary.filter_tokens(bad_ids) # remove words with low frequencies\n",
    "dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldacount = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topicweights = ldacount.inference(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic_distrib = topicweights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic_distrib /= np.sum(doc_topic_distrib,axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1)\n",
    "ax = fig.gca()\n",
    "ax = ax.matshow(doc_topic_distrib, cmap=plt.cm.gray, aspect = '0.05')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_weights.save_topic_weights(datareg,doc_topic_distrib,stem='lemma',package='gensim',rows='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "       self.labels_list = labels_list\n",
    "       self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for doc, label in zip(self.doc_list, self.labels_list):\n",
    "            yield models.doc2vec.TaggedDocument(words=doc,tags=[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['{}'.format(i) for i, _ in enumerate(texts)]\n",
    "sentences = LabeledLineSentence(texts,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = models.Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11051150"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc2vecres = np.zeros((len(texts),100))\n",
    "for i, text in enumerate(texts):\n",
    "    doc2vecres[i,:] = model.infer_vector(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.09356486052274704"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vecres[0,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_weights.save_topic_weights(dataall[idxauthor],doc2vecres,stem=None,package='doc2vec',rows='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'hasinas', 0.7176132202148438),\n",
       " (u'fiorina', 0.6785053014755249),\n",
       " (u'fiorinas', 0.6547480821609497),\n",
       " (u'obamas', 0.6400838494300842),\n",
       " (u'cuomo', 0.6099336743354797),\n",
       " (u'kirchner', 0.6086978912353516),\n",
       " (u'johnson', 0.601860523223877),\n",
       " (u'mccullen', 0.6011120080947876),\n",
       " (u'pathetically', 0.6010297536849976),\n",
       " (u'reagan', 0.593456506729126)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['obama','sander','clinton'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10193164, -0.04376977, -0.13766503,  0.0823018 , -0.02074894,\n",
       "        0.20367937,  0.21111564, -0.05945964,  0.37171602,  0.00507934,\n",
       "       -0.09356486,  0.10768178, -0.07886085, -0.00511813, -0.00253258,\n",
       "        0.04099534,  0.08943591,  0.20622362, -0.02915514, -0.26442051,\n",
       "        0.12644652,  0.04535156,  0.27022526, -0.12068559,  0.03253226,\n",
       "        0.04558017,  0.18684271,  0.19145858,  0.13811885,  0.23620991,\n",
       "        0.19093932, -0.10907025, -0.0671132 ,  0.09404733,  0.01839629,\n",
       "        0.05211755, -0.12075779,  0.19123247,  0.11745412, -0.2026743 ,\n",
       "       -0.02890574,  0.015503  , -0.1583606 ,  0.00218541,  0.00916504,\n",
       "       -0.07925086, -0.28470385, -0.0842441 ,  0.28052595, -0.25641757,\n",
       "       -0.08566134,  0.08883084, -0.2375012 ,  0.12778269, -0.28805211,\n",
       "       -0.08686749,  0.11690425, -0.13369317,  0.2372302 ,  0.01629356,\n",
       "        0.14770885,  0.07089438, -0.2603536 ,  0.16426481, -0.02648615,\n",
       "        0.10391705,  0.0802548 , -0.21442699,  0.20937781, -0.20138375,\n",
       "        0.03252717,  0.29893884, -0.06508335, -0.13541329,  0.12673143,\n",
       "        0.1643908 ,  0.25832537,  0.20783734,  0.25869843, -0.14693005,\n",
       "        0.24352315, -0.12251428,  0.14227477, -0.070927  ,  0.02978115,\n",
       "        0.01555089, -0.51569778,  0.29805702, -0.00896756, -0.09204512,\n",
       "        0.01146462, -0.07057361,  0.24847791, -0.14210543, -0.02696536,\n",
       "        0.02297891,  0.0425211 ,  0.14418139, -0.03502613, -0.32302985], dtype=float32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=iris.target)\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=iris.target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3010\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3010\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3010\n",
      "[t-SNE] Computed conditional probabilities for sample 3010 / 3010\n",
      "[t-SNE] Mean sigma: 0.354876\n",
      "[t-SNE] Error after 100 iterations with early exaggeration: 2.142301\n",
      "[t-SNE] Error after 350 iterations: 1.969357\n"
     ]
    }
   ],
   "source": [
    "X_tsne = TSNE(learning_rate=100,verbose=1).fit_transform(doc2vecres[idxauthor2.as_matrix(),:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "authorid = dataauthor['authorid'][idxauthor][idxauthor2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=authorid[idxauthor].as_matrix())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
