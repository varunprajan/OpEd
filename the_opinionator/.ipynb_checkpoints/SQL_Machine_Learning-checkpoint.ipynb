{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Python packages - you may have to pip install sqlalchemy, sqlalchemy_utils, and psycopg2.\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import topic_weights as tw\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File dataallnew4.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0355a7987420>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load a database from CSV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataallnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dataallnew4.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdatadate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'datadate2.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdataauthor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dataauthor2_38.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dataother2.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/varun/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mfrom_csv\u001b[1;34m(cls, path, header, sep, index_col, parse_dates, encoding, tupleize_cols, infer_datetime_format)\u001b[0m\n\u001b[0;32m   1187\u001b[0m                           \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m                           \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtupleize_cols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m                           infer_datetime_format=infer_datetime_format)\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'block'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/varun/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/varun/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/varun/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/varun/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    797\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 799\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    800\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/varun/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6861)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File dataallnew4.csv does not exist"
     ]
    }
   ],
   "source": [
    "# load a database from CSV\n",
    "dataallnew = pd.DataFrame.from_csv('dataallnew4.csv')\n",
    "datadate = pd.DataFrame.from_csv('datadate2.csv')\n",
    "dataauthor = pd.DataFrame.from_csv('dataauthor2_38.csv')\n",
    "dataother = pd.DataFrame.from_csv('dataother2.csv')\n",
    "topicweights50 = pd.DataFrame.from_csv('topicweights2_50.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 50\n",
    "n_authors = 38\n",
    "n_days = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In Python: Define a database name, and your username for your computer. \n",
    "dbname = 'oped3_db'\n",
    "username = 'varun'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres://varun@localhost/oped3_db\n"
     ]
    }
   ],
   "source": [
    "## 'engine' is a connection to a database\n",
    "## Here, we're using postgres, but sqlalchemy can connect to other things too.\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "print engine.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "## create a database (if it doesn't exist)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## insert data into database from Python (proof of concept - this won't be useful for big data, of course)\n",
    "## df is any pandas dataframe \n",
    "dataallnew.to_sql('orig', engine, if_exists='replace')\n",
    "datadate.to_sql('dates', engine, if_exists='replace')\n",
    "dataauthor.to_sql('authors', engine, if_exists='replace')\n",
    "dataother.to_sql('other', engine, if_exists='replace')\n",
    "topicweights50.to_sql('topic_weights50', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Now try the same queries, but in python!\n",
    "# connect:\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# query:\n",
    "def query_all_2():\n",
    "    return \"\"\"\n",
    "    SELECT * FROM orig\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# query:\n",
    "def query_for_person(firstname,lastname):\n",
    "    return \"\"\"\n",
    "    SELECT orig.share_count, topic_weights50.*, dates.*\n",
    "    FROM orig\n",
    "        JOIN topic_weights50\n",
    "            ON orig.index = topic_weights50.index\n",
    "        JOIN dates\n",
    "            ON orig.index = dates.index\n",
    "    WHERE orig.first_name='{0}' AND orig.last_name='{1}';\n",
    "    \"\"\".format(firstname,lastname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>share_count</th>\n",
       "      <th>index</th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>...</th>\n",
       "      <th>index</th>\n",
       "      <th>Dates</th>\n",
       "      <th>Times</th>\n",
       "      <th>Day0</th>\n",
       "      <th>Day1</th>\n",
       "      <th>Day2</th>\n",
       "      <th>Day3</th>\n",
       "      <th>Day4</th>\n",
       "      <th>Day5</th>\n",
       "      <th>Day6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>665</td>\n",
       "      <td>53</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.223136</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0.995289</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>305</td>\n",
       "      <td>85</td>\n",
       "      <td>0.003805</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.125632</td>\n",
       "      <td>...</td>\n",
       "      <td>85</td>\n",
       "      <td>2016-03-24</td>\n",
       "      <td>0.991755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>412</td>\n",
       "      <td>135</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.024914</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.169041</td>\n",
       "      <td>...</td>\n",
       "      <td>135</td>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>0.987044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>296</td>\n",
       "      <td>177</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.013638</td>\n",
       "      <td>0.048901</td>\n",
       "      <td>...</td>\n",
       "      <td>177</td>\n",
       "      <td>2016-03-16</td>\n",
       "      <td>0.982332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3061</td>\n",
       "      <td>215</td>\n",
       "      <td>0.020640</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>...</td>\n",
       "      <td>215</td>\n",
       "      <td>2016-03-13</td>\n",
       "      <td>0.978799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   share_count  index    topic0    topic1    topic2    topic3    topic4  \\\n",
       "0          665     53  0.000078  0.000078  0.000078  0.000078  0.000078   \n",
       "1          305     85  0.003805  0.000057  0.000057  0.000057  0.000057   \n",
       "2          412    135  0.000072  0.000072  0.000072  0.000072  0.000072   \n",
       "3          296    177  0.000055  0.000055  0.000055  0.000055  0.000055   \n",
       "4         3061    215  0.020640  0.000073  0.000073  0.000073  0.000073   \n",
       "\n",
       "     topic5    topic6    topic7  ...   index       Dates     Times  Day0  \\\n",
       "0  0.000078  0.000078  0.223136  ...      53  2016-03-27  0.995289   0.0   \n",
       "1  0.000057  0.000057  0.125632  ...      85  2016-03-24  0.991755   0.0   \n",
       "2  0.024914  0.000072  0.169041  ...     135  2016-03-20  0.987044   0.0   \n",
       "3  0.000055  0.013638  0.048901  ...     177  2016-03-16  0.982332   0.0   \n",
       "4  0.000073  0.000073  0.000073  ...     215  2016-03-13  0.978799   0.0   \n",
       "\n",
       "   Day1  Day2  Day3  Day4  Day5  Day6  \n",
       "0   0.0   0.0   0.0   0.0   0.0   1.0  \n",
       "1   0.0   0.0   1.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   1.0  \n",
       "3   0.0   1.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   1.0  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charlesblowquery = query_for_person('charles','blow')\n",
    "rossdouthatquery = query_for_person('ross','douthat')\n",
    "charlesblow = pd.read_sql_query(charlesblowquery,con)\n",
    "rossdouthat = pd.read_sql_query(rossdouthatquery,con)\n",
    "rossdouthat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query:\n",
    "def query_all():\n",
    "    return \"\"\"\n",
    "    SELECT orig.share_count, orig.first_name, orig.last_name, topic_weights50.*, authors.*, dates.*, other.*\n",
    "    FROM orig\n",
    "        LEFT JOIN topic_weights50\n",
    "            ON orig.index = topic_weights50.index\n",
    "        LEFT JOIN authors\n",
    "            ON orig.index = authors.index\n",
    "        LEFT JOIN dates\n",
    "            ON orig.index = dates.index\n",
    "        LEFT JOIN other\n",
    "            ON orig.index = other.index\n",
    "    WHERE authors.author0 = 0\n",
    "    \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5767, 107)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query = query_all()\n",
    "data_from_sql = pd.read_sql_query(sql_query,con)\n",
    "data_from_sql.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_from_sql['log_share_count'] = np.log10(data_from_sql['share_count'])\n",
    "data_from_sql[data_from_sql['log_share_count'] < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_figure(fig,aspect=None,xlabel=None,ylabel=None,axisbounds=None,fontsizeaxes=36,fontsizeother=27,\n",
    "                  ticksize=False,borderwidth=5,tight=True,tightlayout=True,tightfac=1.08,ticksizedef=[18,5]):\n",
    "    '''Prettifies a figure with labels, proper linewidths and font sizes, ticks, tight axes/layout, etc.\n",
    "    Note that for the axisbounds, there are three options: direct specification (axisbounds)\n",
    "    tight = True (tight axes), or tight = False (tight axes with small padding around outside).\n",
    "    Also, aspect should be set to 1 for plots of physical objects'''\n",
    "    ax = fig.axes[0]\n",
    "    if xlabel is not None:\n",
    "        ax.set_xlabel(xlabel,fontsize=fontsizeaxes)\n",
    "    if ylabel is not None:\n",
    "        ax.set_ylabel(ylabel,fontsize=fontsizeaxes)\n",
    "    ax.legend(loc='NorthWest',fontsize=fontsizeother)\n",
    "    if axisbounds is not None:\n",
    "        ax.axis(axisbounds)\n",
    "    else:\n",
    "        ax.autoscale()\n",
    "        ax.axis('tight')\n",
    "        if not tight:\n",
    "            xcenter = np.mean(ax.get_xlim())\n",
    "            ycenter = np.mean(ax.get_ylim())\n",
    "            xlimnottight = [(x-xcenter)*tightfac + xcenter for x in ax.get_xlim()]\n",
    "            ylimnottight = [(y-ycenter)*tightfac + ycenter for y in ax.get_ylim()]\n",
    "            ax.axis(xlimnottight + ylimnottight)\n",
    "    if aspect is not None:\n",
    "        ax.set_aspect(aspect)\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax.spines[axis].set_linewidth(borderwidth)\n",
    "    if ticksize is None:\n",
    "        ax.tick_params(axis='both',labelsize=fontsizeother,bottom='off',top='off',left='off',right='off')\n",
    "    else:\n",
    "        if not ticksize:\n",
    "            ticksize = ticksizedef      \n",
    "        ax.tick_params(axis='both',labelsize=fontsizeother,width=ticksize[1],length=ticksize[0])\n",
    "    if tightlayout:\n",
    "        fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pd_plot(df,xlabel,ylabels,ynames,ylabel,window=None):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    df2 = pd.DataFrame(index=df.index,columns=['x','y'])\n",
    "    df2['x'] = df[xlabel]\n",
    "    for ylabelcurr, yname in zip(ylabels,ynames):\n",
    "        if window is None:\n",
    "            df2['y'] = df[ylabelcurr]\n",
    "        else:\n",
    "            df2['y'] = pd.rolling_mean(df[ylabelcurr],window=window)\n",
    "        df2.plot(ax=ax,x='x',y='y',linewidth=5,label=yname)\n",
    "    fig = pretty_figure(fig,xlabel='Date',ylabel=ylabel)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:10: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=8,center=False).mean()\n"
     ]
    }
   ],
   "source": [
    "pd_plot(rossdouthat[::-1],'Dates',['topic14','topic44','topic47'],['General politics','Republican politics','Culture/social issues'],ylabel='Topic composition',window=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:10: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=8,center=False).mean()\n"
     ]
    }
   ],
   "source": [
    "pd_plot(charlesblow[::-1],'Dates',['topic4','topic10','topic47','topic48'],['Race/policing','Presidency','Culture/social issues','Inequality'],ylabel='Topic composition',window=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_plot(rossdouthat[::-1],xlabel='Dates',ylabels=['share_count'],ynames=['Shares'],ylabel='Share count',window=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_plot(data_from_sql[::-1],xlabel='Dates',ylabels=['share_count'],ynames=['Shares'],ylabel='Share count',window=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, neighbors, ensemble, preprocessing\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(data,frac=0.7):\n",
    "    datanew = data.sample(frac=1)\n",
    "    nrows = len(datanew)\n",
    "    idx = int(nrows*frac)\n",
    "    return {'train': datanew.iloc[:idx], 'test': datanew.iloc[idx:]}\n",
    "    # use 70-30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(data,featurenames,viralityname,model,errorfun,**kwargs):\n",
    "    splitdata = split_data(data)\n",
    "    datatrain = splitdata['train']\n",
    "    datatest = splitdata['test']\n",
    "    model.fit(datatrain[featurenames],datatrain[viralityname],**kwargs)\n",
    "    train_pred = model.predict(datatrain[featurenames])\n",
    "    test_pred = model.predict(datatest[featurenames])\n",
    "    train_error = errorfun(train_pred,datatrain[viralityname])\n",
    "    test_error = errorfun(test_pred,datatest[viralityname])\n",
    "    print('Training error: {0}'.format(train_error))\n",
    "    print('Test error: {0}'.format(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topicnames = ['topic{0}'.format(i) for i in range(n_topics)]\n",
    "authornames = ['author{0}'.format(i) for i in range(n_authors)]\n",
    "daynames = ['Day{0}'.format(i) for i in range(n_days)]\n",
    "# othernames = ['len','Times']\n",
    "# othernames = ['Times']\n",
    "othernames = []\n",
    "featurenames = topicnames + authornames + daynames + othernames\n",
    "features = data_from_sql[featurenames]\n",
    "viralityname = 'log_share_count'\n",
    "virality = data_from_sql[viralityname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin Data according to Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percentile = 90.\n",
    "dec = percentile/100.\n",
    "weight = dec/(1.-dec)\n",
    "cutoff = np.percentile(data_from_sql[viralityname],percentile)\n",
    "classassignments = [1 if virality > cutoff else 0 for virality in data_from_sql[viralityname]]\n",
    "classweightdict = {1: weight, 0: 1}\n",
    "weights = [classweightdict[classassignment] for classassignment in classassignments]\n",
    "data_from_sql['class'] = classassignments\n",
    "data_from_sql['weight'] = weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasplit = split_data(data_from_sql,frac=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Weighting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(class_weight=classweightdict)\n",
    "model2 = ensemble.RandomForestClassifier(max_depth=10,class_weight=classweightdict)\n",
    "model3 = ensemble.BaggingClassifier(bootstrap=True,bootstrap_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mymodel = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train', 0.78737426292056889)\n",
      "('Test', 0.76907073509015256)\n"
     ]
    }
   ],
   "source": [
    "datatrain = datasplit['train']\n",
    "mymodel.fit(datatrain[featurenames],datatrain['class'])\n",
    "res = mymodel.predict(datatrain[featurenames])\n",
    "print('Train',accuracy_score(datatrain['class'], res))\n",
    "datatest = datasplit['test']\n",
    "probas = mymodel.predict_proba(datatest[featurenames])\n",
    "res = mymodel.predict(datatest[featurenames])\n",
    "print('Test',accuracy_score(datatest['class'], res))\n",
    "fpr, tpr, thresholds = roc_curve(datatest['class'], probas[:, 1])\n",
    "prec, rec, thresh  = precision_recall_curve(datatest['class'], probas[:, 1])\n",
    "thresh = np.append(thresh,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_fig(fpr,tpr,thresh1,prec,rec,thresh2,fignum=1,option='roc'):\n",
    "    fig = plt.figure(fignum)\n",
    "    ax = fig.gca()\n",
    "    if option == 'roc':\n",
    "        x = [fpr]\n",
    "        y = [tpr]\n",
    "    elif option == 'precrec':\n",
    "        x = [rec]\n",
    "        y = [prec]\n",
    "    elif option == 'precrecthresh':\n",
    "        x = [thresh2,thresh2]\n",
    "        y = [prec,rec]\n",
    "    for xcurr, ycurr in zip(x,y):\n",
    "        ax.plot(xcurr,ycurr)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_fig(fpr,tpr,thresholds,prec,rec,thresh,option='precrec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1)\n",
    "ax = fig.gca()\n",
    "ax.plot(thresh, prec)\n",
    "ax.plot(thresh, rec)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(2)\n",
    "ax = fig.gca()\n",
    "ax.plot(rec,prec)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77284550026010057"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.fit(data_from_sql[featurenames],data_from_sql['class'])\n",
    "probas = mymodel.predict_proba(data_from_sql[featurenames])\n",
    "res = mymodel.predict(data_from_sql[featurenames])\n",
    "fpr, tpr, thresholds = roc_curve(data_from_sql['class'], probas[:, 1])\n",
    "accuracy_score(data_from_sql['class'], res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1)\n",
    "ax = fig.gca()\n",
    "ax = ax.matshow(data_from_sql[featurenames], cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = linear_model.LogisticRegression()\n",
    "model.fit(datatrain[featurenames],datatrain['bin_share_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = neighbors.KNeighbors(n_neighbors=i)\n",
    "train_model(datasplit,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_error_regr(predicted,actual):\n",
    "    nobs = actual.size\n",
    "    return np.sqrt(1.0/(2.0*nobs)*np.sum((actual - predicted)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coeff_regr(predicted,actual):\n",
    "    avg = np.mean(actual)\n",
    "    sstot = np.sum((actual - avg)**2)\n",
    "    ssres = np.sum((actual - predicted)**2)\n",
    "    return 1 - ssres/sstot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = linear_model.Ridge(alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_from_sql[featurenames],data_from_sql[viralityname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in np.abs(model.coef_).argsort():\n",
    "    pass\n",
    "    # print(featurenames[i],model.coef_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.4791213694\n",
      "Test error: 0.471185117435\n"
     ]
    }
   ],
   "source": [
    "train_model(data_from_sql,featurenames,viralityname,model,coeff_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('topic0', 2.9814248414746345)\n",
      "('topic1', 3.6076759018200617)\n",
      "('topic2', 2.7145149878369748)\n",
      "('topic3', 2.2318192563795209)\n",
      "('topic4', 3.6101955737904237)\n",
      "('topic5', 3.1324073977838909)\n",
      "('topic6', 3.1952031055581167)\n",
      "('topic7', 3.0161974794220709)\n",
      "('topic8', 2.437344841580555)\n",
      "('topic9', 2.5022442184888116)\n",
      "('topic10', 3.1384742739679439)\n",
      "('topic11', 3.0884752722055806)\n",
      "('topic12', 2.9326794368848952)\n",
      "('topic13', -13.152639268231551)\n",
      "('topic14', 2.8038590260980301)\n",
      "('topic15', 2.6980174493417461)\n",
      "('topic16', 2.48883448964876)\n",
      "('topic17', 3.0652225875645875)\n",
      "('topic18', -13.152639268231551)\n",
      "('topic19', 2.5818149085043389)\n",
      "('topic20', 2.3225718410000273)\n",
      "('topic21', 3.0046006094774205)\n",
      "('topic22', -13.152639268231546)\n",
      "('topic23', 1.6082704382912816)\n",
      "('topic24', -13.152639268231539)\n",
      "('topic25', -13.152639268231541)\n",
      "('topic26', 3.4973596041867774)\n",
      "('topic27', 3.1588957615861792)\n",
      "('topic28', 2.9229127997798914)\n",
      "('topic29', 6.3575888248040355)\n",
      "('topic30', 2.3580108238481632)\n",
      "('topic31', 2.830188204728596)\n",
      "('topic32', 2.6480252215059386)\n",
      "('topic33', -13.152639268231541)\n",
      "('topic34', 3.0825137661993582)\n",
      "('topic35', 2.0596013323581195)\n",
      "('topic36', -13.152639268231543)\n",
      "('topic37', 3.1704253639881443)\n",
      "('topic38', 2.9146377636211804)\n",
      "('topic39', 2.7993068352993684)\n",
      "('topic40', -13.152639268231541)\n",
      "('topic41', 0.24903681425395779)\n",
      "('topic42', 2.0247563320332884)\n",
      "('topic43', 2.4773038641090994)\n",
      "('topic44', 3.2422444097114904)\n",
      "('topic45', -13.152639268231546)\n",
      "('topic46', 2.584018440671799)\n",
      "('topic47', 3.189967386236134)\n",
      "('topic48', 3.2941233045557552)\n",
      "('topic49', 2.65315320207108)\n",
      "('author0', 0.0)\n",
      "('author1', -0.423458441741003)\n",
      "('author2', -0.20931237563667912)\n",
      "('author3', 0.48290962812680965)\n",
      "('author4', -0.10895459729981184)\n",
      "('author5', -0.035175406892665635)\n",
      "('author6', 0.019965247967677933)\n",
      "('author7', -0.064342037342908318)\n",
      "('author8', 0.095270983098244499)\n",
      "('author9', 0.056348028553087674)\n",
      "('author10', -0.23799252446700048)\n",
      "('author11', 0.14512805490879041)\n",
      "('author12', -0.1039267392819586)\n",
      "('author13', 0.096752769339993869)\n",
      "('author14', 0.052391242014860437)\n",
      "('author15', -0.23033330719913261)\n",
      "('author16', 0.42870296480632508)\n",
      "('author17', 0.67895565736285945)\n",
      "('author18', -0.14471983341335529)\n",
      "('author19', 0.10428832075227658)\n",
      "('author20', -0.52692901340387954)\n",
      "('author21', -0.70547691082012598)\n",
      "('author22', 0.3326469328268008)\n",
      "('author23', -0.39367126903818855)\n",
      "('author24', 0.88830404906499516)\n",
      "('author25', -0.45228717489508824)\n",
      "('author26', -1.2988377672390792)\n",
      "('author27', 0.11046757483610473)\n",
      "('author28', 0.2443042227989243)\n",
      "('author29', 0.2785193028538302)\n",
      "('author30', -0.2139120458899533)\n",
      "('author31', -0.41014746603118857)\n",
      "('author32', 0.29856833165370517)\n",
      "('author33', -0.1572580735119555)\n",
      "('author34', 0.43414003038151472)\n",
      "('author35', -0.87837188892581775)\n",
      "('author36', -0.13833483551004552)\n",
      "('author37', 0.28794291147062084)\n",
      "('Day0', -0.22947697962082311)\n",
      "('Day1', -0.25039829037085143)\n",
      "('Day2', -0.23416927116839908)\n",
      "('Day3', -0.25990371612276852)\n",
      "('Day4', -0.27947510930226466)\n",
      "('Day5', -0.32157841542506171)\n",
      "('Day6', -0.12283367361667678)\n"
     ]
    }
   ],
   "source": [
    "for featurename, coef in zip(featurenames, model.coef_):\n",
    "    print(featurename, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = neighbors.KNeighborsRegressor(n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.525892577549\n",
      "Test error: 0.380602064415\n"
     ]
    }
   ],
   "source": [
    "train_model(datasplit,model,coeff_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ensemble.RandomForestRegressor(n_estimators=200,max_features='sqrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.927809274534\n",
      "Test error: 0.445940914378\n"
     ]
    }
   ],
   "source": [
    "train_model(datasplit,model,coeff_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Ridge' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-dadadd207663>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimportance\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturenames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;31m# print(feature, importance)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Ridge' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "for feature, importance in zip(featurenames,model.feature_importances_):\n",
    "    # print(feature, importance)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.998883446246\n",
      "Test error: -0.147600334939\n",
      "Training error: 0.716377900879\n",
      "Test error: 0.152592380258\n",
      "Training error: 0.619053854171\n",
      "Test error: 0.243409621316\n",
      "Training error: 0.569168974685\n",
      "Test error: 0.290740795355\n",
      "Training error: 0.535852162635\n",
      "Test error: 0.314018356802\n",
      "Training error: 0.515676841609\n",
      "Test error: 0.328033807347\n",
      "Training error: 0.500432695738\n",
      "Test error: 0.332794871677\n",
      "Training error: 0.488118540344\n",
      "Test error: 0.34068243082\n",
      "Training error: 0.478990621509\n",
      "Test error: 0.345101970322\n",
      "Training error: 0.469109110887\n",
      "Test error: 0.344840873636\n",
      "Training error: 0.461235157144\n",
      "Test error: 0.34593098155\n",
      "Training error: 0.454972515161\n",
      "Test error: 0.350557689176\n",
      "Training error: 0.448723296798\n",
      "Test error: 0.351332730917\n",
      "Training error: 0.444368263776\n",
      "Test error: 0.353331101435\n",
      "Training error: 0.439076912761\n",
      "Test error: 0.355747969173\n",
      "Training error: 0.437369664485\n",
      "Test error: 0.355988512055\n",
      "Training error: 0.43284801782\n",
      "Test error: 0.352899477027\n",
      "Training error: 0.429192621953\n",
      "Test error: 0.35384234248\n",
      "Training error: 0.424704593041\n",
      "Test error: 0.353192479167\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,20):\n",
    "    model = neighbors.KNeighborsRegressor(n_neighbors=i)\n",
    "    train_model(datasplit,model,coeff_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickleizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickleizer.save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = pickleizer.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual = data_from_sql[viralityname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = model.predict(data_from_sql[featurenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(actual,pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Make Some Predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import model_predict\n",
    "import similar_articles\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of the_opinionator.views failed: Traceback (most recent call last):\n",
      "  File \"/home/varun/anaconda2/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "AssertionError: View function mapping is overwriting an existing endpoint function: index\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "authorid = model_predict.AUTHORID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://www.nytimes.com/2016/06/03/opinion/the-id-that-ate-the-planet.html'\n",
    "firstname = 'nicholas'\n",
    "lastname = 'kristof'\n",
    "dayofweek = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predict.predict_new_article_url(firstname,lastname,dayofweek,url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fulltext = u'Trump trumped Trump yesterday'\n",
    "weights = model_predict.article_weights(fulltext)\n",
    "df = similar_articles.return_df()\n",
    "sameauthor, otherauthor = similar_articles.same_other_df(df,firstname,lastname)\n",
    "expectedshares = model_predict.predict_new_article_text(firstname,lastname,dayofweek,weights)\n",
    "percsame, percother = similar_articles.percentiles(sameauthor,otherauthor,expectedshares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predict.predict_new_article_url(firstname,lastname,dayofweek,url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of the_opinionator.views failed: Traceback (most recent call last):\n",
      "  File \"/home/varun/anaconda2/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "AssertionError: View function mapping is overwriting an existing endpoint function: index\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "dataall = pd.DataFrame.from_csv('dataall3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for firstname, lastname in zip(dataall['first_name'],dataall['last_name']):\n",
    "    if type(firstname) == float and type(lastname) == float:\n",
    "        res.append('The Editorial Board')\n",
    "    elif type(firstname) == float or type(lastname) == float:\n",
    "        res.append('Other')\n",
    "    else:\n",
    "        res.append('{0} {1}'.format(firstname.title(),lastname.title()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataall['author'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataall['Date'] = datadate['Dates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_authors_id(dataall,n_authors):\n",
    "    groupedauthors = dataall.groupby('author').size()\n",
    "    groupedauthors['Other'] = 0\n",
    "    topauthors = groupedauthors.sort_values(ascending=False)[:n_authors]\n",
    "    topid = [1 if author in topauthors else 0 for author in dataall['author']]\n",
    "    dataall['topid'] = topid\n",
    "    return dataall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataall = top_authors_id(dataall,12)\n",
    "datatop = dataall[dataall['topid'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'comment_count', u'document_type', u'first_name', u'full_text',\n",
       "       u'last_name', u'like_count', u'share_count', u'url', u'Title',\n",
       "       u'author', u'Date', u'topid'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatop.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datafinal = datatop[['share_count','author','Date','Title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datafinal.rename(columns = {'share_count':'Share Count', 'author': 'Author'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafinal.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obsolete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('test.out','wb') as f:\n",
    "    for row in datatop.iterrows():\n",
    "        linewrite = []\n",
    "        for key in ['Date','Share Count','Author']:\n",
    "            val = row[1][key]\n",
    "            linewrite.append(\"'{0}': '{1}'\".format(key,val))\n",
    "        lineall = '{' + ', '.join(linewrite)+ '},' + '\\n'\n",
    "        f.write(lineall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similar Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import similar_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"SELECT * FROM orig\",similar_articles.con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\n    SELECT orig.index, orig.first_name, orig.last_name, orig.url, orig.Title, orig.share_count, topic_weights50.*\n    FROM orig\n        LEFT JOIN topic_weights50\n            ON orig.index = topic_weights50.index\n    ': column orig.title does not exist\nLINE 2: ...index, orig.first_name, orig.last_name, orig.url, orig.Title...\n                                                             ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a98613977d2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msql_query\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimilar_articles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_similar_articles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Paul'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Krugman'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/varun/similar_articles.pyc\u001b[0m in \u001b[0;36mpredict_similar_articles\u001b[1;34m(weights, firstname, lastname, n_articles)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict_similar_articles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfirstname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlastname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_articles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0msql_query\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimilarity_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql_query\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfirstname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlastname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'first_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'last_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/varun/anaconda2/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mread_sql_query\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[0;32m    429\u001b[0m     return pandas_sql.read_query(\n\u001b[0;32m    430\u001b[0m         \u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 431\u001b[1;33m         parse_dates=parse_dates, chunksize=chunksize)\n\u001b[0m\u001b[0;32m    432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/varun/anaconda2/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mread_query\u001b[1;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[0;32m   1597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1599\u001b[1;33m         \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1600\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/varun/anaconda2/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1574\u001b[0m             ex = DatabaseError(\n\u001b[0;32m   1575\u001b[0m                 \"Execution failed on sql '%s': %s\" % (args[0], exc))\n\u001b[1;32m-> 1576\u001b[1;33m             \u001b[0mraise_with_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1578\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/varun/anaconda2/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1563\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '\n    SELECT orig.index, orig.first_name, orig.last_name, orig.url, orig.Title, orig.share_count, topic_weights50.*\n    FROM orig\n        LEFT JOIN topic_weights50\n            ON orig.index = topic_weights50.index\n    ': column orig.title does not exist\nLINE 2: ...index, orig.first_name, orig.last_name, orig.url, orig.Title...\n                                                             ^\n"
     ]
    }
   ],
   "source": [
    "sql_query = similar_articles.predict_similar_articles(np.array([1,2]),'Paul','Krugman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
