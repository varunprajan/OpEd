{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Python packages - you may have to pip install sqlalchemy, sqlalchemy_utils, and psycopg2.\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import topic_weights as tw\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of the_opinionator.views failed: Traceback (most recent call last):\n",
      "  File \"/home/varun/anaconda2/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "AssertionError: View function mapping is overwriting an existing endpoint function: index\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# load a database from CSV\n",
    "dataall = pd.DataFrame.from_csv('dataall3.csv')\n",
    "datadate = pd.DataFrame.from_csv('datadate2.csv')\n",
    "dataauthor = pd.DataFrame.from_csv('dataauthor2_38.csv')\n",
    "dataother = pd.DataFrame.from_csv('dataother2.csv')\n",
    "topicweights50 = pd.DataFrame.from_csv('topicweights2_50.csv')\n",
    "topicweights100 = pd.DataFrame.from_csv(\n",
    "    tw.topic_weights_csv(100,stem='lemma',package='gensim',rows='reg'))\n",
    "topicweights50_2 = pd.DataFrame.from_csv(\n",
    "    tw.topic_weights_csv(50,stem='lemma',package='gensim',rows='reg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicweights100_2 = pd.DataFrame.from_csv(\n",
    "    tw.topic_weights_csv(100,stem=None,package='doc2vec',rows='reg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 50\n",
    "n_authors = 38\n",
    "n_days = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In Python: Define a database name, and your username for your computer. \n",
    "dbname = 'oped_db'\n",
    "username = 'varun'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres://varun@localhost/oped_db\n"
     ]
    }
   ],
   "source": [
    "## 'engine' is a connection to a database\n",
    "## Here, we're using postgres, but sqlalchemy can connect to other things too.\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "print engine.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "## create a database (if it doesn't exist)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataall.to_sql('orig', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## insert data into database from Python (proof of concept - this won't be useful for big data, of course)\n",
    "## df is any pandas dataframe \n",
    "dataall.to_sql('orig', engine, if_exists='replace')\n",
    "datadate.to_sql('dates', engine, if_exists='replace')\n",
    "dataauthor.to_sql('authors', engine, if_exists='replace')\n",
    "dataother.to_sql('other', engine, if_exists='replace')\n",
    "topicweights50.to_sql('topic_weights50', engine, if_exists='replace')\n",
    "topicweights100.to_sql('topic_weights100', engine, if_exists='replace')\n",
    "topicweights50_2.to_sql('topic_weights50_2', engine, if_exists='replace')\n",
    "topicweights100_2.to_sql('topic_weights100_2', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Now try the same queries, but in python!\n",
    "# connect:\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# query:\n",
    "def query_for_person(firstname,lastname):\n",
    "    return \"\"\"\n",
    "    SELECT orig.share_count, topic_weights100.*, dates.*\n",
    "    FROM orig\n",
    "        JOIN topic_weights100\n",
    "            ON orig.index = topic_weights100.index\n",
    "        JOIN dates\n",
    "            ON orig.index = dates.index\n",
    "    WHERE orig.first_name='{0}' AND orig.last_name='{1}';\n",
    "    \"\"\".format(firstname,lastname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>share_count</th>\n",
       "      <th>index</th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>...</th>\n",
       "      <th>index</th>\n",
       "      <th>Dates</th>\n",
       "      <th>Times</th>\n",
       "      <th>Day0</th>\n",
       "      <th>Day1</th>\n",
       "      <th>Day2</th>\n",
       "      <th>Day3</th>\n",
       "      <th>Day4</th>\n",
       "      <th>Day5</th>\n",
       "      <th>Day6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>665</td>\n",
       "      <td>53</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.054906</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0.995289</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>305</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>...</td>\n",
       "      <td>85</td>\n",
       "      <td>2016-03-24</td>\n",
       "      <td>0.991755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>412</td>\n",
       "      <td>135</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.024739</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.006673</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>135</td>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>0.987044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>296</td>\n",
       "      <td>177</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>177</td>\n",
       "      <td>2016-03-16</td>\n",
       "      <td>0.982332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3061</td>\n",
       "      <td>215</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>215</td>\n",
       "      <td>2016-03-13</td>\n",
       "      <td>0.978799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   share_count  index    topic0    topic1    topic2    topic3    topic4  \\\n",
       "0          665     53  0.000022  0.054906  0.000022  0.000022  0.000022   \n",
       "1          305     85  0.000017  0.000017  0.000017  0.000017  0.000017   \n",
       "2          412    135  0.000023  0.024739  0.000023  0.006673  0.000023   \n",
       "3          296    177  0.000018  0.000018  0.000018  0.000018  0.000018   \n",
       "4         3061    215  0.000023  0.000023  0.000023  0.000023  0.000023   \n",
       "\n",
       "     topic5    topic6    topic7  ...   index       Dates     Times  Day0  \\\n",
       "0  0.000022  0.000022  0.000022  ...      53  2016-03-27  0.995289   0.0   \n",
       "1  0.000017  0.000017  0.000017  ...      85  2016-03-24  0.991755   0.0   \n",
       "2  0.000023  0.000023  0.000023  ...     135  2016-03-20  0.987044   0.0   \n",
       "3  0.000018  0.000018  0.000018  ...     177  2016-03-16  0.982332   0.0   \n",
       "4  0.000023  0.000023  0.000023  ...     215  2016-03-13  0.978799   0.0   \n",
       "\n",
       "   Day1  Day2  Day3  Day4  Day5  Day6  \n",
       "0   0.0   0.0   0.0   0.0   0.0   1.0  \n",
       "1   0.0   0.0   1.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   1.0  \n",
       "3   0.0   1.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   1.0  \n",
       "\n",
       "[5 rows x 112 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charlesblowquery = query_for_person('charles','blow')\n",
    "rossdouthatquery = query_for_person('ross','douthat')\n",
    "charlesblow = pd.read_sql_query(charlesblowquery,con)\n",
    "rossdouthat = pd.read_sql_query(rossdouthatquery,con)\n",
    "rossdouthat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query:\n",
    "def query_all():\n",
    "    return \"\"\"\n",
    "    SELECT orig.share_count, orig.first_name, orig.last_name, topic_weights50.*, authors.*, dates.*, other.*\n",
    "    FROM orig\n",
    "        LEFT JOIN topic_weights50\n",
    "            ON orig.index = topic_weights50.index\n",
    "        LEFT JOIN authors\n",
    "            ON orig.index = authors.index\n",
    "        LEFT JOIN dates\n",
    "            ON orig.index = dates.index\n",
    "        LEFT JOIN other\n",
    "            ON orig.index = other.index\n",
    "    WHERE authors.author0 = 0\n",
    "    \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5767, 106)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query = query_all()\n",
    "data_from_sql = pd.read_sql_query(sql_query,con)\n",
    "data_from_sql.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_from_sql['log_share_count'] = np.log10(data_from_sql['share_count'])\n",
    "data_from_sql[data_from_sql['log_share_count'] < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_figure(fig,aspect=None,xlabel=None,ylabel=None,axisbounds=None,fontsizeaxes=36,fontsizeother=27,\n",
    "                  ticksize=False,borderwidth=5,tight=True,tightlayout=True,tightfac=1.08,ticksizedef=[18,5]):\n",
    "    '''Prettifies a figure with labels, proper linewidths and font sizes, ticks, tight axes/layout, etc.\n",
    "    Note that for the axisbounds, there are three options: direct specification (axisbounds)\n",
    "    tight = True (tight axes), or tight = False (tight axes with small padding around outside).\n",
    "    Also, aspect should be set to 1 for plots of physical objects'''\n",
    "    ax = fig.axes[0]\n",
    "    if xlabel is not None:\n",
    "        ax.set_xlabel(xlabel,fontsize=fontsizeaxes)\n",
    "    if ylabel is not None:\n",
    "        ax.set_ylabel(ylabel,fontsize=fontsizeaxes)\n",
    "    ax.legend(loc='NorthWest',fontsize=fontsizeother)\n",
    "    if axisbounds is not None:\n",
    "        ax.axis(axisbounds)\n",
    "    else:\n",
    "        ax.autoscale()\n",
    "        ax.axis('tight')\n",
    "        if not tight:\n",
    "            xcenter = np.mean(ax.get_xlim())\n",
    "            ycenter = np.mean(ax.get_ylim())\n",
    "            xlimnottight = [(x-xcenter)*tightfac + xcenter for x in ax.get_xlim()]\n",
    "            ylimnottight = [(y-ycenter)*tightfac + ycenter for y in ax.get_ylim()]\n",
    "            ax.axis(xlimnottight + ylimnottight)\n",
    "    if aspect is not None:\n",
    "        ax.set_aspect(aspect)\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax.spines[axis].set_linewidth(borderwidth)\n",
    "    if ticksize is None:\n",
    "        ax.tick_params(axis='both',labelsize=fontsizeother,bottom='off',top='off',left='off',right='off')\n",
    "    else:\n",
    "        if not ticksize:\n",
    "            ticksize = ticksizedef      \n",
    "        ax.tick_params(axis='both',labelsize=fontsizeother,width=ticksize[1],length=ticksize[0])\n",
    "    if tightlayout:\n",
    "        fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pd_plot(df,xlabel,ylabels,ynames,ylabel,window=None):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    df2 = pd.DataFrame(index=df.index,columns=['x','y'])\n",
    "    df2['x'] = df[xlabel]\n",
    "    for ylabelcurr, yname in zip(ylabels,ynames):\n",
    "        if window is None:\n",
    "            df2['y'] = df[ylabelcurr]\n",
    "        else:\n",
    "            df2['y'] = pd.rolling_mean(df[ylabelcurr],window=window)\n",
    "        df2.plot(ax=ax,x='x',y='y',linewidth=5,label=yname)\n",
    "    fig = pretty_figure(fig,xlabel='Date',ylabel=ylabel)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:10: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=8,center=False).mean()\n"
     ]
    }
   ],
   "source": [
    "pd_plot(rossdouthat[::-1],'Dates',['topic14','topic44','topic47'],['General politics','Republican politics','Culture/social issues'],ylabel='Topic composition',window=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:10: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=8,center=False).mean()\n"
     ]
    }
   ],
   "source": [
    "pd_plot(charlesblow[::-1],'Dates',['topic4','topic10','topic47','topic48'],['Race/policing','Presidency','Culture/social issues','Inequality'],ylabel='Topic composition',window=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_plot(rossdouthat[::-1],xlabel='Dates',ylabels=['share_count'],ynames=['Shares'],ylabel='Share count',window=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_plot(data_from_sql[::-1],xlabel='Dates',ylabels=['share_count'],ynames=['Shares'],ylabel='Share count',window=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, neighbors, ensemble, preprocessing\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(data,frac=0.7):\n",
    "    datanew = data.sample(frac=1)\n",
    "    nrows = len(datanew)\n",
    "    idx = int(nrows*frac)\n",
    "    return {'train': datanew.iloc[:idx], 'test': datanew.iloc[idx:]}\n",
    "    # use 70-30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(split_data,model,errorfun,**kwargs):\n",
    "    datatrain = split_data['train']\n",
    "    datatest = split_data['test']\n",
    "    model.fit(datatrain[featurenames],datatrain[viralityname],**kwargs)\n",
    "    train_pred = model.predict(datatrain[featurenames])\n",
    "    test_pred = model.predict(datatest[featurenames])\n",
    "    train_error = errorfun(train_pred,datatrain[viralityname])\n",
    "    test_error = errorfun(test_pred,datatest[viralityname])\n",
    "    print('Training error: {0}'.format(train_error))\n",
    "    print('Test error: {0}'.format(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topicnames = ['topic{0}'.format(i) for i in range(n_topics)]\n",
    "authornames = ['author{0}'.format(i) for i in range(n_authors)]\n",
    "daynames = ['Day{0}'.format(i) for i in range(n_days)]\n",
    "# othernames = ['len','Times']\n",
    "# othernames = ['Times']\n",
    "othernames = []\n",
    "featurenames = topicnames + authornames + daynames + othernames\n",
    "features = data_from_sql[featurenames]\n",
    "viralityname = 'log_share_count'\n",
    "virality = data_from_sql[viralityname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin Data according to Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percentile = 90.\n",
    "dec = percentile/100.\n",
    "weight = dec/(1.-dec)\n",
    "cutoff = np.percentile(data_from_sql[viralityname],percentile)\n",
    "classassignments = [1 if virality > cutoff else 0 for virality in data_from_sql[viralityname]]\n",
    "classweightdict = {1: weight, 0: 1}\n",
    "weights = [classweightdict[classassignment] for classassignment in classassignments]\n",
    "data_from_sql['class'] = classassignments\n",
    "data_from_sql['weight'] = weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasplit = split_data(data_from_sql,frac=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Weighting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(class_weight=classweightdict)\n",
    "model2 = ensemble.RandomForestClassifier(max_depth=10,class_weight=classweightdict)\n",
    "model3 = ensemble.BaggingClassifier(bootstrap=True,bootstrap_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mymodel = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train', 0.78737426292056889)\n",
      "('Test', 0.76907073509015256)\n"
     ]
    }
   ],
   "source": [
    "datatrain = datasplit['train']\n",
    "mymodel.fit(datatrain[featurenames],datatrain['class'])\n",
    "res = mymodel.predict(datatrain[featurenames])\n",
    "print('Train',accuracy_score(datatrain['class'], res))\n",
    "datatest = datasplit['test']\n",
    "probas = mymodel.predict_proba(datatest[featurenames])\n",
    "res = mymodel.predict(datatest[featurenames])\n",
    "print('Test',accuracy_score(datatest['class'], res))\n",
    "fpr, tpr, thresholds = roc_curve(datatest['class'], probas[:, 1])\n",
    "prec, rec, thresh  = precision_recall_curve(datatest['class'], probas[:, 1])\n",
    "thresh = np.append(thresh,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_fig(fpr,tpr,thresh1,prec,rec,thresh2,fignum=1,option='roc'):\n",
    "    fig = plt.figure(fignum)\n",
    "    ax = fig.gca()\n",
    "    if option == 'roc':\n",
    "        x = [fpr]\n",
    "        y = [tpr]\n",
    "    elif option == 'precrec':\n",
    "        x = [rec]\n",
    "        y = [prec]\n",
    "    elif option == 'precrecthresh':\n",
    "        x = [thresh2,thresh2]\n",
    "        y = [prec,rec]\n",
    "    for xcurr, ycurr in zip(x,y):\n",
    "        ax.plot(xcurr,ycurr)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_fig(fpr,tpr,thresholds,prec,rec,thresh,option='precrec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1)\n",
    "ax = fig.gca()\n",
    "ax.plot(thresh, prec)\n",
    "ax.plot(thresh, rec)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(2)\n",
    "ax = fig.gca()\n",
    "ax.plot(rec,prec)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77284550026010057"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.fit(data_from_sql[featurenames],data_from_sql['class'])\n",
    "probas = mymodel.predict_proba(data_from_sql[featurenames])\n",
    "res = mymodel.predict(data_from_sql[featurenames])\n",
    "fpr, tpr, thresholds = roc_curve(data_from_sql['class'], probas[:, 1])\n",
    "accuracy_score(data_from_sql['class'], res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1)\n",
    "ax = fig.gca()\n",
    "ax = ax.matshow(data_from_sql[featurenames], cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = linear_model.LogisticRegression()\n",
    "model.fit(datatrain[featurenames],datatrain['bin_share_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = neighbors.KNeighbors(n_neighbors=i)\n",
    "train_model(datasplit,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_error_regr(predicted,actual):\n",
    "    nobs = actual.size\n",
    "    return np.sqrt(1.0/(2.0*nobs)*np.sum((actual - predicted)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coeff_regr(predicted,actual):\n",
    "    avg = np.mean(actual)\n",
    "    sstot = np.sum((actual - avg)**2)\n",
    "    ssres = np.sum((actual - predicted)**2)\n",
    "    return 1 - ssres/sstot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = linear_model.Ridge(alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_from_sql[authornames],data_from_sql[viralityname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in np.abs(model.coef_).argsort():\n",
    "    pass\n",
    "    # print(featurenames[i],model.coef_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.497754277174\n",
      "Test error: 0.428288401756\n"
     ]
    }
   ],
   "source": [
    "train_model(datasplit,model,coeff_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for featurename, coef in zip(featurenames, model.coef_):\n",
    "    # print(featurename, coef)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = neighbors.KNeighborsRegressor(n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.537220246398\n",
      "Test error: 0.404536228443\n"
     ]
    }
   ],
   "source": [
    "train_model(datasplit,model,coeff_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ensemble.RandomForestRegressor(n_estimators=200,max_features='sqrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.927809274534\n",
      "Test error: 0.445940914378\n"
     ]
    }
   ],
   "source": [
    "train_model(datasplit,model,coeff_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Ridge' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-dadadd207663>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimportance\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturenames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;31m# print(feature, importance)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Ridge' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "for feature, importance in zip(featurenames,model.feature_importances_):\n",
    "    # print(feature, importance)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.998883446246\n",
      "Test error: -0.147600334939\n",
      "Training error: 0.716377900879\n",
      "Test error: 0.152592380258\n",
      "Training error: 0.619053854171\n",
      "Test error: 0.243409621316\n",
      "Training error: 0.569168974685\n",
      "Test error: 0.290740795355\n",
      "Training error: 0.535852162635\n",
      "Test error: 0.314018356802\n",
      "Training error: 0.515676841609\n",
      "Test error: 0.328033807347\n",
      "Training error: 0.500432695738\n",
      "Test error: 0.332794871677\n",
      "Training error: 0.488118540344\n",
      "Test error: 0.34068243082\n",
      "Training error: 0.478990621509\n",
      "Test error: 0.345101970322\n",
      "Training error: 0.469109110887\n",
      "Test error: 0.344840873636\n",
      "Training error: 0.461235157144\n",
      "Test error: 0.34593098155\n",
      "Training error: 0.454972515161\n",
      "Test error: 0.350557689176\n",
      "Training error: 0.448723296798\n",
      "Test error: 0.351332730917\n",
      "Training error: 0.444368263776\n",
      "Test error: 0.353331101435\n",
      "Training error: 0.439076912761\n",
      "Test error: 0.355747969173\n",
      "Training error: 0.437369664485\n",
      "Test error: 0.355988512055\n",
      "Training error: 0.43284801782\n",
      "Test error: 0.352899477027\n",
      "Training error: 0.429192621953\n",
      "Test error: 0.35384234248\n",
      "Training error: 0.424704593041\n",
      "Test error: 0.353192479167\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,20):\n",
    "    model = neighbors.KNeighborsRegressor(n_neighbors=i)\n",
    "    train_model(datasplit,model,coeff_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickleizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickleizer.save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = pickleizer.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual = data_from_sql[viralityname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = model.predict(data_from_sql[featurenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(actual,pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Make Some Predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import model_predict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of the_opinionator.views failed: Traceback (most recent call last):\n",
      "  File \"/home/varun/anaconda2/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "AssertionError: View function mapping is overwriting an existing endpoint function: index\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "authorid = model_predict.AUTHORID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://www.nytimes.com/2016/06/03/opinion/the-id-that-ate-the-planet.html'\n",
    "firstname = 'nicholas'\n",
    "lastname = 'kristof'\n",
    "dayofweek = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predict.predict_new_article_url(firstname,lastname,dayofweek,url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predict.predict_new_article_url(firstname,lastname,dayofweek,url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of the_opinionator.views failed: Traceback (most recent call last):\n",
      "  File \"/home/varun/anaconda2/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "AssertionError: View function mapping is overwriting an existing endpoint function: index\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "dataall = pd.DataFrame.from_csv('dataall3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for firstname, lastname in zip(dataall['first_name'],dataall['last_name']):\n",
    "    if type(firstname) == float and type(lastname) == float:\n",
    "        res.append('The Editorial Board')\n",
    "    elif type(firstname) == float or type(lastname) == float:\n",
    "        res.append('Other')\n",
    "    else:\n",
    "        res.append('{0} {1}'.format(firstname.title(),lastname.title()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataall['author'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataall['Date'] = datadate['Dates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_authors_id(dataall,n_authors):\n",
    "    groupedauthors = dataall.groupby('author').size()\n",
    "    groupedauthors['Other'] = 0\n",
    "    topauthors = groupedauthors.sort_values(ascending=False)[:n_authors]\n",
    "    topid = [1 if author in topauthors else 0 for author in dataall['author']]\n",
    "    dataall['topid'] = topid\n",
    "    return dataall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataall = top_authors_id(dataall,12)\n",
    "datatop = dataall[dataall['topid'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'comment_count', u'document_type', u'first_name', u'full_text',\n",
       "       u'last_name', u'like_count', u'share_count', u'url', u'Title',\n",
       "       u'author', u'Date', u'topid'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatop.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datafinal = datatop[['share_count','author','Date','Title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datafinal.rename(columns = {'share_count':'Share Count', 'author': 'Author'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafinal.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obsolete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('test.out','wb') as f:\n",
    "    for row in datatop.iterrows():\n",
    "        linewrite = []\n",
    "        for key in ['Date','Share Count','Author']:\n",
    "            val = row[1][key]\n",
    "            linewrite.append(\"'{0}': '{1}'\".format(key,val))\n",
    "        lineall = '{' + ', '.join(linewrite)+ '},' + '\\n'\n",
    "        f.write(lineall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
